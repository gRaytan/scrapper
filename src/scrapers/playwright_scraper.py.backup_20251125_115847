"""Playwright-based scraper implementation."""
import asyncio
import hashlib
import xml.etree.ElementTree as ET
import httpx
import requests

from datetime import datetime
from dateutil import parser as date_parser
from typing import List, Dict, Any, Optional

from bs4 import BeautifulSoup
from playwright.async_api import async_playwright, Browser, Page

from .base_scraper import BaseScraper
from .parsers import ComeetParser, GreenhouseParser, AmazonParser, EightfoldParser, SmartRecruitersParser, RSSParser, MetaParser, SalesforceParser, JibeParser, PhenomParser
from src.utils.logger import logger


class PlaywrightScraper(BaseScraper):
    """Scraper using Playwright for dynamic content."""
    
    def __init__(self, company_config: Dict[str, Any], scraping_config: Dict[str, Any], **kwargs):
        super().__init__(company_config, scraping_config, **kwargs)
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None
        self.playwright = None
        self.comeet_parser = ComeetParser()
        self.greenhouse_parser = GreenhouseParser()
        self.amazon_parser = AmazonParser()
        self.eightfold_parser = EightfoldParser()
        self.smartrecruiters_parser = SmartRecruitersParser()
        self.rss_parser = RSSParser()
        self.meta_parser = MetaParser()
        self.salesforce_parser = SalesforceParser()
        self.jibe_parser = JibeParser()
        self.phenom_parser = PhenomParser()
    
    async def setup(self):
        """Initialize Playwright browser."""
        logger.info("Setting up Playwright browser")
        self.playwright = await async_playwright().start()
        
        # Launch browser with stealth options
        self.browser = await self.playwright.chromium.launch(
            headless=True,
            args=[
                '--disable-blink-features=AutomationControlled',
                '--disable-dev-shm-usage',
                '--no-sandbox',
            ]
        )
        
        # Create context with realistic settings
        context = await self.browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        )
        
        self.page = await context.new_page()
        logger.success("Playwright browser ready")
    
    async def teardown(self):
        """Close Playwright browser."""
        logger.info("Closing Playwright browser")
        if self.page:
            await self.page.close()
        if self.browser:
            await self.browser.close()
        if self.playwright:
            await self.playwright.stop()
    
    async def scrape(self) -> List[Dict[str, Any]]:
        """
        Main scraping method using Playwright.
        
        Returns:
            List of job dictionaries
        """
        jobs = []
        
        try:
            # Check if this is Meta (uses GraphQL) - check early
            company_name = self.company_config.get("name", "")
            if company_name == "Meta":
                jobs = await self._scrape_meta_graphql()
                logger.success(f"Scraped {len(jobs)} jobs from Meta GraphQL")
                self.stats["jobs_found"] = len(jobs)
                return jobs
            
            # Check if using simple HTTP scraping (for Island) or RSS (for Palo Alto)
            scraper_type = self.scraping_config.get("scraper_type", "playwright")
            if scraper_type == "requests":
                jobs = await self._scrape_html()
                logger.success(f"Scraped {len(jobs)} jobs")
                self.stats["jobs_found"] = len(jobs)
                return jobs
            elif scraper_type == "rss":
                jobs = await self._scrape_rss()
                logger.success(f"Scraped {len(jobs)} jobs")
                self.stats["jobs_found"] = len(jobs)
                return jobs
            elif scraper_type == "workday":
                jobs = await self._scrape_workday()
                logger.success(f"Scraped {len(jobs)} jobs")
                self.stats["jobs_found"] = len(jobs)
                return jobs
            elif scraper_type == "phenom":
                jobs = await self._scrape_phenom()
                logger.success(f"Scraped {len(jobs)} jobs")
                self.stats["jobs_found"] = len(jobs)
                return jobs
            elif scraper_type == "api":
                # Check if pagination is needed
                pagination_type = self.scraping_config.get("pagination_type", "offset")
                if pagination_type == "none":
                    jobs = await self._scrape_api()
                else:
                    jobs = await self._scrape_api_with_pagination()
                logger.success(f"Scraped {len(jobs)} jobs")
                self.stats["jobs_found"] = len(jobs)
                return jobs

            pagination_type = self.scraping_config.get("pagination_type", "none")
            
            if pagination_type == "api":
                # API-based scraping (e.g., Monday.com)
                jobs = await self._scrape_api()
            elif pagination_type == "dynamic":
                # Dynamic page scraping (e.g., Microsoft)
                jobs = await self._scrape_dynamic()
            else:
                # Standard scraping
                jobs = await self._scrape_standard()
            
            logger.success(f"Scraped {len(jobs)} jobs")
            self.stats["jobs_found"] = len(jobs)
            

        except Exception as e:
            logger.error(f"Error during scraping: {e}")
            self.stats["errors"] += 1
            raise
        
        return jobs
    
    async def _scrape_api(self) -> List[Dict[str, Any]]:
        """Scrape jobs from API endpoint."""
        api_endpoint = self.scraping_config.get("api_endpoint")
        api_params = self.scraping_config.get("api_params", {})

        logger.info(f"Fetching jobs from API: {api_endpoint}")
        logger.info(f"API params: {api_params}")

        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.get(api_endpoint, params=api_params)
            logger.info(f"API Response Status: {response.status_code}")
            logger.info(f"API Response Headers: {dict(response.headers)}")

            response.raise_for_status()
            data = response.json()

            # Log the response structure for debugging
            logger.info(f"API Response Type: {type(data)}")
            if isinstance(data, dict):
                logger.info(f"API Response Keys: {list(data.keys())}")
                logger.info(f"Full API Response: {data}")
            else:
                logger.info(f"API Response (first 500 chars): {str(data)[:500]}")

        jobs = []

        # Detect API format and parse accordingly
        positions = []
        parser_func = None
        
        if isinstance(data, dict) and "jobs" in data:
            # Check if it's Jibe format (jobs wrapped in "data" key) or Greenhouse
            sample_job = data["jobs"][0] if data["jobs"] else {}
            if isinstance(sample_job, dict) and "data" in sample_job:
                # Jibe API format (used by Booking.com)
                positions = data["jobs"]
                parser_func = self.jibe_parser.parse
                logger.info(f"Detected Jibe API format with {len(positions)} jobs")
            else:
                # Greenhouse API format
                positions = data["jobs"]
                parser_func = self.greenhouse_parser.parse
                logger.info(f"Detected Greenhouse API format with {len(positions)} jobs")
            logger.info(f"Detected Greenhouse API format with {len(positions)} jobs")
        elif isinstance(data, list):
            # Comeet API format (list directly)
            positions = data
            parser_func = self.comeet_parser.parse
            logger.info(f"Detected Comeet API format (list) with {len(positions)} positions")
        elif isinstance(data, dict) and "positions" in data:
            # Comeet API format (dict with 'positions' key)
            positions = data["positions"]
            parser_func = self.comeet_parser.parse
            logger.info(f"Detected Comeet API format (dict) with {len(positions)} positions")
        else:
            logger.warning(f"Unexpected API response format. Expected Greenhouse or Comeet format, got: {type(data)}")
            logger.warning(f"Response keys: {list(data.keys()) if isinstance(data, dict) else 'N/A'}")
            self.stats["requests_made"] += 1
            return jobs

        for position in positions:
            job = parser_func(position)
            if job:
                logger.info(f"Parsed job: {job.get('title')} at {job.get('location')}")
                if self.validate_job_data(job):
                    # Apply location filter
                    if self.matches_location_filter(job):
                        jobs.append(self.normalize_job_data(job))
                    else:
                        self.stats["jobs_filtered"] += 1
                        logger.debug(f'Filtered out job: {job.get("title")} at {job.get("location")}')
                else:
                    logger.warning(f"Job failed validation: {job}")
            else:
                position_name = position.get('name') or position.get('title', 'Unknown')
                logger.warning(f"Failed to parse position: {position_name}")

        self.stats["requests_made"] += 1
        return jobs
    
    def _make_absolute_url(self, href: str) -> str:
        """Convert relative URL to absolute URL."""
        if href.startswith("http://") or href.startswith("https://"):
            return href
        
        base_url = self.company_config.get("website", "")
        if not base_url:
            return href
        
        # Remove trailing slash from base_url
        base_url = base_url.rstrip("/")
        
        if href.startswith("/"):
            return f"{base_url}{href}"
        else:
            return f"{base_url}/{href}"
    
    async def _extract_jobs_from_page(self, selectors: Dict[str, str]) -> List[Dict[str, Any]]:
        """Extract job listings from current page."""
        jobs = []

        job_item_selector = selectors.get("job_item")
        if not job_item_selector:
            logger.warning("No job_item selector configured")
            return jobs

        logger.info(f"Looking for job items with selector: {job_item_selector}")

        # Get all job items
        job_elements = await self.page.query_selector_all(job_item_selector)
        logger.info(f"Found {len(job_elements)} job elements with selector: {job_item_selector}")

        if len(job_elements) == 0:
            # Try to debug what's on the page
            logger.warning("No job elements found. Trying to find what's on the page...")

            # Try each selector individually
            for selector_name, selector_value in selectors.items():
                if selector_value:
                    try:
                        elements = await self.page.query_selector_all(selector_value)
                        logger.info(f"Selector '{selector_name}' ({selector_value}): found {len(elements)} elements")
                    except Exception as e:
                        logger.error(f"Error testing selector '{selector_name}' ({selector_value}): {e}")

        for idx, element in enumerate(job_elements):
            try:
                job = await self._extract_job_from_element(element, selectors)
                logger.info(f"Job {idx + 1}: {job}")
                if job and self.validate_job_data(job):
                    jobs.append(self.normalize_job_data(job))
                else:
                    logger.warning(f"Job {idx + 1} failed validation: {job}")
            except Exception as e:
                logger.error(f"Error extracting job {idx + 1}: {e}")
                self.stats["errors"] += 1

        return jobs
    
    async def _extract_job_from_element(
        self,
        element,
        selectors: Dict[str, str]
    ) -> Dict[str, Any]:
        """Extract job data from a single element."""
        job = {}
        
        # Extract title
        title_selector = selectors.get("job_title")
        if title_selector:
            title_el = await element.query_selector(title_selector)
            if title_el:
                job["title"] = (await title_el.inner_text()).strip()
        
        # Extract location
        location_selector = selectors.get("job_location")
        if location_selector:
            location_el = await element.query_selector(location_selector)
            if location_el:
                job["location"] = (await location_el.inner_text()).strip()
        
        # Extract URL - try multiple strategies
        url_found = False
        
        # Strategy 1: Check if title element is a link
        if title_el:
            tag_name = await title_el.evaluate("el => el.tagName.toLowerCase()")
            if tag_name == "a":
                href = await title_el.get_attribute("href")
                if href:
                    job["job_url"] = self._make_absolute_url(href)
                    url_found = True
        
        # Strategy 2: Look for link using URL selector
        if not url_found:
            url_selector = selectors.get("job_url")
            if url_selector:
                url_el = await element.query_selector(url_selector)
                if url_el:
                    href = await url_el.get_attribute("href")
                    if href:
                        job["job_url"] = self._make_absolute_url(href)
                        url_found = True
        
        # Strategy 3: Look for any link in the element
        if not url_found:
            link_el = await element.query_selector("a[href]")
            if link_el:
                href = await link_el.get_attribute("href")
                if href and ("/job" in href.lower() or "/career" in href.lower() or "/position" in href.lower()):
                    job["job_url"] = self._make_absolute_url(href)
                    url_found = True

        # Extract department
        dept_selector = selectors.get("job_department")
        if dept_selector:
            dept_el = await element.query_selector(dept_selector)
            if dept_el:
                job["department"] = (await dept_el.inner_text()).strip()

        # Extract employment type
        type_selector = selectors.get("job_type")
        if type_selector:
            type_el = await element.query_selector(type_selector)
            if type_el:
                job["employment_type"] = (await type_el.inner_text()).strip()

        # Generate external_id from URL or title
        if "job_url" in job:
            # Use last part of URL as ID
            job["external_id"] = job["job_url"].split("/")[-1].split("?")[0]
        elif "title" in job:
            # Generate hash from title
            job["external_id"] = hashlib.md5(job["title"].encode()).hexdigest()[:16]

        return job



    async def _scrape_dynamic(self) -> List[Dict[str, Any]]:
        """Scrape jobs from dynamic page (e.g., Microsoft)."""
        careers_url = self.company_config.get("careers_url")
        search_url = self.scraping_config.get("search_url", careers_url)

        logger.info(f"Navigating to {search_url}")
        # Increased timeout to 90 seconds for slow-loading pages
        await self.page.goto(search_url, wait_until="domcontentloaded", timeout=90000)

        # Save page HTML for debugging
        html_content = await self.page.content()
        logger.info(f"Page loaded, HTML length: {len(html_content)} characters")

        # Save to file for inspection
        import os
        debug_dir = "data/raw"
        os.makedirs(debug_dir, exist_ok=True)
        debug_file = os.path.join(debug_dir, f"{self.company_config.get('name', 'unknown').replace(' ', '_')}_page.html")
        with open(debug_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        logger.info(f"Saved page HTML to {debug_file}")

        # Wait for job listings to load
        wait_selector = self.scraping_config.get("wait_for_selector")
        if wait_selector:
            try:
                await self.page.wait_for_selector(wait_selector, timeout=30000)
                logger.info(f"Found selector: {wait_selector}")
            except Exception as e:
                logger.warning(f"Timeout waiting for selector {wait_selector}: {e}")
                logger.info(f"Page title: {await self.page.title()}")
                logger.info(f"Page URL: {self.page.url}")

        jobs = []
        selectors = self.scraping_config.get("selectors", {})

        # Extract jobs from current page
        jobs_on_page = await self._extract_jobs_from_page(selectors)
        jobs.extend(jobs_on_page)
        
        self.stats["pages_scraped"] += 1
        self.stats["requests_made"] += 1
        return jobs
    
    async def _scrape_standard(self) -> List[Dict[str, Any]]:
        """Standard scraping for regular pages."""
        careers_url = self.company_config.get("careers_url")
        
        logger.info(f"Navigating to {careers_url}")
        await self.page.goto(careers_url, wait_until="networkidle", timeout=60000)
        
        jobs = []
        selectors = self.scraping_config.get("selectors", {})
        
        # Extract jobs from current page
        jobs_on_page = await self._extract_jobs_from_page(selectors)
        jobs.extend(jobs_on_page)
        
        self.stats["pages_scraped"] += 1
        self.stats["requests_made"] += 1
        return jobs

    async def _scrape_html(self) -> List[Dict[str, Any]]:
        """
        Scrape jobs using simple HTTP requests (for sites like Island that block Playwright).
        
        Returns:
            List of job dictionaries
        """
        
        careers_url = self.company_config.get("careers_url")
        logger.info(f"Fetching HTML from: {careers_url}")
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        }
        
        try:
            response = requests.get(careers_url, headers=headers, timeout=30)
            response.raise_for_status()
            logger.info(f"HTTP Status: {response.status_code}")
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Get selectors from config
            selectors = self.scraping_config.get("selectors", {})
            job_link_selector = selectors.get("job_link", "a[href*='comeet.com/jobs']")
            
            # Find all job links
            job_links = soup.select(job_link_selector)
            logger.info(f"Found {len(job_links)} job links")
            
            jobs = []
            seen_positions = set()
            
            for link in job_links:
                href = link.get('href', '')
                position = link.get('data-position', '')
                
                # Skip if no position or duplicate
                if not position or position in seen_positions:
                    continue
                seen_positions.add(position)
                
                # Extract location and department
                location = link.get('data-location', '')
                department = link.get('data-team', '')
                
                # If not in link, try parent container
                if not location or not department:
                    parent = link.find_parent(attrs={'data-location': True})
                    if parent:
                        location = location or parent.get('data-location', '')
                        department = department or parent.get('data-team', '')
                
                # Make URL absolute
                if href and not href.startswith('http'):
                    if href.startswith('/'):
                        href = 'https://www.comeet.com' + href
                    else:
                        href = 'https://www.comeet.com/' + href
                
                # Create job dict
                job = {
                    "title": position,
                    "location": location,
                    "department": department,
                    "job_url": href,
                    "description": "",
                    "employment_type": None,
                    "posted_date": None,
                    "is_remote": "remote" in location.lower() if location else False,
                }
                
                # Validate and normalize
                if self.validate_job_data(job):
                    # Apply location filter
                    if self.matches_location_filter(job):
                        jobs.append(self.normalize_job_data(job))
                        logger.info(f"Parsed job: {job['title']} at {job['location']}")
                    else:
                        self.stats["jobs_filtered"] += 1
                        logger.debug(f'Filtered out job: {job["title"]} at {job["location"]}')
                else:
                    logger.warning(f"Job failed validation: {job}")
            
            self.stats["requests_made"] += 1
            return jobs
            
        except Exception as e:
            logger.error(f"Error scraping HTML: {e}")
            self.stats["errors"] += 1
            return []

    async def _scrape_rss(self) -> List[Dict[str, Any]]:
        """Scrape jobs from RSS/XML feed (e.g., TalentBrew)."""
        
        rss_url = self.scraping_config.get("rss_url")
        if not rss_url:
            logger.error("No rss_url configured for RSS scraping")
            return []
        
        logger.info(f"Fetching jobs from RSS feed: {rss_url}")
        
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.get(rss_url)
                response.raise_for_status()
                
                logger.info(f"RSS Response Status: {response.status_code}")
                logger.info(f"Content-Type: {response.headers.get('content-type')}")
            
            # Parse XML
            root = ET.fromstring(response.content)
            
            # Find all job items
            items = root.findall('.//item')
            logger.info(f"Found {len(items)} jobs in RSS feed")
            
            jobs = []
            for item in items:
                job = self.rss_parser.parse(item)
                if job:
                    logger.info(f"Parsed job: {job.get('title')} at {job.get('location')}")
                    if self.validate_job_data(job):
                        # Apply location filter
                        if self.matches_location_filter(job):
                            jobs.append(self.normalize_job_data(job))
                        else:
                            self.stats["jobs_filtered"] += 1
                            logger.debug(f'Filtered out job: {job.get("title")} at {job.get("location")}')
                    else:
                        logger.warning(f"Job failed validation: {job}")
                else:
                    logger.warning(f"Failed to parse RSS item")
            
            self.stats["requests_made"] += 1
            return jobs
            
        except Exception as e:
            logger.error(f"Error scraping RSS feed: {e}")
            self.stats["errors"] += 1
            return []
    
    async def _scrape_api_with_pagination(self) -> List[Dict[str, Any]]:
        """Scrape jobs from API with pagination support (e.g., Amazon, Nvidia Eightfold)."""
        api_endpoint = self.scraping_config.get("api_endpoint")
        pagination_params = self.scraping_config.get("pagination_params", {})
        query_params = self.scraping_config.get("query_params", {})
        response_structure = self.scraping_config.get("response_structure", {})
        
        offset_param = pagination_params.get("offset_param", "offset")
        limit_param = pagination_params.get("limit_param", "limit")
        page_size = pagination_params.get("page_size", 100)
        max_pages = pagination_params.get("max_pages", 10)  # Limit to prevent infinite loops
        
        # Get jobs key (supports nested keys like "data.positions")
        jobs_key = response_structure.get("jobs_key", "jobs")
        total_key = response_structure.get("total_key", "hits")
        
        logger.info(f"Fetching jobs from API with pagination: {api_endpoint}")
        logger.info(f"Pagination: {offset_param}={page_size}, max_pages={max_pages}")
        
        all_jobs = []
        offset = 0
        page = 0
        
        while page < max_pages:
            # Build params for this page
            params = dict(query_params)  # Start with base query params
            params[offset_param] = offset
            if limit_param:  # Only add limit if specified (Eightfold doesn't use it)
                params[limit_param] = page_size
            
            logger.info(f"Fetching page {page + 1} (offset={offset}, limit={page_size})")
            
            try:
                async with httpx.AsyncClient(timeout=30.0) as client:
                    response = await client.get(api_endpoint, params=params)
                    response.raise_for_status()
                    data = response.json()
                
                # Parse jobs based on API format
                jobs_on_page = []
                
                # Extract positions from response (supports nested keys)
                positions = self._extract_nested_value(data, jobs_key)
                
                if positions and isinstance(positions, list):
                    # Get total hits if available
                    total_hits = self._extract_nested_value(data, total_key) if total_key else 0
                    
                    logger.info(f"Found {len(positions)} jobs on page {page + 1}" + 
                               (f" (total: {total_hits})" if total_hits else ""))
                    
                    # Determine which parser to use
                    company_name = self.company_config.get("name", "")
                    
                    for position in positions:
                        # Use company-specific parser
                        if company_name == "Nvidia":
                            job = self.eightfold_parser.parse(position)
                        elif company_name == "Wix":
                            job = self.smartrecruiters_parser.parse(position)
                        elif company_name == "Amazon":
                            job = self.amazon_parser.parse(position)
                        elif jobs_key == "jobs" or "greenhouse" in api_endpoint.lower():
                            # Greenhouse API format (jobs_key is "jobs" or URL contains "greenhouse")
                            job = self.greenhouse_parser.parse(position)
                        else:
                            logger.warning(f"No parser defined for company: {company_name}")
                            continue
                        if job and self.validate_job_data(job):
                            # Apply location filter
                            if self.matches_location_filter(job):
                                jobs_on_page.append(self.normalize_job_data(job))
                            else:
                                self.stats["jobs_filtered"] += 1
                                logger.debug(f'Filtered out job: {job.get("title")} at {job.get("location")}')

                    all_jobs.extend(jobs_on_page)
                    self.stats["requests_made"] += 1
                    
                    # Check if we've reached the end
                    if len(positions) == 0:
                        logger.info(f"No more jobs found at page {page + 1}")
                        break
                    elif len(positions) < page_size:
                        logger.info(f"Reached end of results at page {page + 1}")
                        break
                    elif total_hits and offset + len(positions) >= total_hits:
                        logger.info(f"Reached total hits ({total_hits}) at page {page + 1}")
                        break
                else:
                    logger.warning(f"Unexpected API format or no positions found")
                    logger.warning(f"Response keys: {list(data.keys()) if isinstance(data, dict) else type(data)}")
                    break
                
                # Move to next page
                offset += len(positions)  # Use actual number of positions returned
                page += 1
                
                # Small delay between requests
                await asyncio.sleep(self.scraping_config.get("wait_time", 1))
                
            except Exception as e:
                logger.error(f"Error fetching page {page + 1}: {e}")
                self.stats["errors"] += 1
                break
        
        logger.info(f"Total jobs scraped: {len(all_jobs)} across {page + 1} pages")
        return all_jobs
    
    def _extract_nested_value(self, data: Dict[str, Any], key_path: str) -> Any:
        """Extract value from nested dictionary using dot notation (e.g., 'data.positions')."""
        if not key_path:
            return None
        
        keys = key_path.split('.')
        value = data
        
        for key in keys:
            if isinstance(value, dict) and key in value:
                value = value[key]
            else:
                return None
        
        return value
    
    async def _scrape_workday(self) -> List[Dict[str, Any]]:
        """Scrape jobs from Workday API with pagination support (e.g., Salesforce)."""
        api_endpoint = self.scraping_config.get("api_endpoint")
        pagination_params = self.scraping_config.get("pagination_params", {})
        workday_config = self.scraping_config.get("workday_config", {})
        response_structure = self.scraping_config.get("response_structure", {})
        
        offset_param = pagination_params.get("offset_param", "offset")
        limit_param = pagination_params.get("limit_param", "limit")
        page_size = pagination_params.get("page_size", 20)
        max_pages = pagination_params.get("max_pages", 50)
        
        # Workday-specific config
        search_text = workday_config.get("search_text", "")
        applied_facets = workday_config.get("applied_facets", {})
        
        # Get jobs key
        jobs_key = response_structure.get("jobs_key", "jobPostings")
        total_key = response_structure.get("total_key", "total")
        
        logger.info(f"Fetching jobs from Workday API: {api_endpoint}")
        logger.info(f"Search text: {search_text}")
        logger.info(f"Pagination: {offset_param}={page_size}, max_pages={max_pages}")
        
        all_jobs = []
        offset = 0
        page = 0
        
        while page < max_pages:
            # Build Workday POST payload
            payload = {
                "appliedFacets": applied_facets,
                limit_param: page_size,
                offset_param: offset,
                "searchText": search_text
            }
            
            logger.info(f"Fetching page {page + 1} (offset={offset}, limit={page_size})")
            
            try:
                async with httpx.AsyncClient(timeout=30.0) as client:
                    response = await client.post(
                        api_endpoint,
                        json=payload,
                        headers={
                            'Content-Type': 'application/json',
                            'Accept': 'application/json'
                        }
                    )
                    response.raise_for_status()
                    data = response.json()
                
                # Extract job postings
                positions = data.get(jobs_key, [])
                total_hits = data.get(total_key, 0)
                
                if positions and isinstance(positions, list):
                    logger.info(f"Found {len(positions)} jobs on page {page + 1} (total: {total_hits})")
                    
                    jobs_on_page = []
                    for position in positions:
                        # Use Salesforce/Workday parser
                        job = self.salesforce_parser.parse(position)
                        
                        if job and self.validate_job_data(job):
                            # Apply location filter
                            if self.matches_location_filter(job):
                                jobs_on_page.append(self.normalize_job_data(job))
                            else:
                                self.stats["jobs_filtered"] += 1
                                logger.debug(f'Filtered out job: {job.get("title")} at {job.get("location")}')
                    
                    all_jobs.extend(jobs_on_page)
                    self.stats["requests_made"] += 1
                    
                    # Check if we've reached the end
                    if len(positions) == 0:
                        logger.info(f"No more jobs found at page {page + 1}")
                        break
                    elif len(positions) < page_size:
                        logger.info(f"Reached end of results at page {page + 1}")
                        break
                    elif offset + len(positions) >= total_hits:
                        logger.info(f"Reached total hits ({total_hits}) at page {page + 1}")
                        break
                else:
                    logger.warning(f"No positions found in response")
                    break
                
                # Move to next page
                offset += len(positions)
                page += 1
                
                # Small delay between requests
                await asyncio.sleep(self.scraping_config.get("wait_time", 1))
                
            except Exception as e:
                logger.error(f"Error fetching page {page + 1}: {e}")
                self.stats["errors"] += 1
                break
        
        logger.info(f"Total jobs scraped: {len(all_jobs)} across {page + 1} pages")
        return all_jobs



    async def _scrape_phenom(self) -> List[Dict[str, Any]]:
        """Scrape jobs from Phenom People platform using Playwright.
        
        Returns:
            List of job dictionaries
        """
        careers_url = self.company_config.get("careers_url")
        phenom_config = self.scraping_config.get("phenom_config", {})
        
        # Get location filter from config
        location_filter = phenom_config.get("location_filter", "")
        max_pages = phenom_config.get("max_pages", 5)
        
        logger.info(f"Scraping Phenom People platform: {careers_url}")
        if location_filter:
            logger.info(f"Location filter: {location_filter}")
        
        # Build URL with location filter
        search_url = careers_url
        if location_filter:
            # Check if we should use path-based or query parameter
            location_param_type = phenom_config.get("location_param_type", "query")
            if location_param_type == "path":
                # Path-based: /search-jobs/Location (Intuit)
                search_url = f"{careers_url}/{location_filter}"
            else:
                # Query parameter: ?location=Location (ServiceNow)
                search_url = f"{careers_url}?location={location_filter}"
        
        logger.info(f"Navigating to {search_url}")
        await self.page.goto(search_url, wait_until="domcontentloaded", timeout=60000)
        await self.page.wait_for_timeout(10000)  # Wait for jobs to load
        
        # Scroll to load more jobs
        for i in range(3):
            await self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            await self.page.wait_for_timeout(2000)
        
        all_jobs = []
        seen_urls = set()
        
        # Find all job links
        # Try multiple selectors for different platforms
        job_links = []
        selectors = [
    'a[href*="/job/"]',  # Intuit/Phenom - try this first
    'a[class*="job"]',  # ServiceNow
    'h2 a',  # ServiceNow alternative
        ]
        
        for selector in selectors:
            try:
                links = await self.page.query_selector_all(selector)
                if links:
                    job_links = links
                    logger.info(f"Using selector: {selector}")
                    break
            except Exception as e:
                logger.debug(f"Selector {selector} failed: {e}")
        logger.info(f"Found {len(job_links)} job links")
        
        for link in job_links:
            try:
                href = await link.get_attribute('href')
                if href and href not in seen_urls and ('/job/' in href or '/jobs/' in href):
                    seen_urls.add(href)
                    
                    # Get the link text (title)
                    title = await link.inner_text()
                    title = title.strip()
                    
                    # Check if this is a real job title (not navigation)
                    if title and len(title) > 5 and 'saved' not in title.lower() and 'alert' not in title.lower() and 'jobs' != title.lower():
                        # Try to find location from nearby elements
                        location = "Unknown"
                        try:
                            # Try to find parent container and look for location
                            parent = await link.evaluate_handle('el => el.closest("article, div[class*=\'job\'], li")')
                            if parent:
                                # Look for location text in parent
                                location_text = await parent.evaluate(
                                    '''el => { const locationEl = el.querySelector('[class*="location"], [class*="city"], .card-text, .list-inline'); return locationEl ? locationEl.textContent.trim() : null; }'''
                                )
                                if location_text:
                                    location = location_text
                        except Exception as e:
                            logger.debug(f"Could not extract location from parent: {e}")
                        
                        # If location not found, try to extract from URL
                        if location == "Unknown":
                            import re
                            # Match /job/{location}/{title}/ pattern (not /jobs/{id}/{title}/)
                            location_match = re.search(r'/job/([^/]+)/', href)
                            if location_match:
                                potential_location = location_match.group(1)
                                # Skip if it's just a number (job ID)
                                if not potential_location.isdigit():
                                    location = potential_location.replace('-', ' ').title()
                        
                        # Build full URL
                        job_url = href if href.startswith('http') else f"{careers_url.rstrip('/')}{href}"
                        
                        job_data = {
                            'title': title,
                            'location': location,
                            'url': job_url
                        }
                        
                        # Parse using Phenom parser
                        job = self.phenom_parser.parse(job_data)
                        
                        if job and self.validate_job_data(job):
                            # Apply location filter
                            if self.matches_location_filter(job):
                                all_jobs.append(self.normalize_job_data(job))
                            else:
                                self.stats["jobs_filtered"] += 1
                                logger.debug(f'Filtered out job: {job.get("title")} at {job.get("location")}')
            except Exception as e:
                logger.warning(f"Error extracting job from link: {e}")
                continue
        
        logger.info(f"Total jobs scraped: {len(all_jobs)}")
        self.stats["requests_made"] += 1
        return all_jobs

    async def _scrape_meta_graphql(self) -> List[Dict[str, Any]]:
        """
        Scrape Meta careers using Playwright to intercept GraphQL responses.
        
        Returns:
            List of job dictionaries
        """
        jobs = []
        search_url = self.scraping_config.get("search_url") or self.company_config.get("careers_url")
        
        logger.info(f"Scraping Meta GraphQL from: {search_url}")
        
        # Variable to store GraphQL response
        graphql_jobs_data = None
        
        async def handle_response(response):
            """Intercept GraphQL responses to extract job data."""
            nonlocal graphql_jobs_data
            
            if 'graphql' in response.url:
                try:
                    data = await response.json()
                    if 'data' in data and data['data']:
                        if 'job_search_with_featured_jobs' in data['data']:
                            graphql_jobs_data = data['data']['job_search_with_featured_jobs']
                            logger.info("Captured Meta GraphQL jobs response")
                except Exception as e:
                    logger.debug(f"Error parsing GraphQL response: {e}")
        
        try:
            # Set up response handler
            self.page.on('response', handle_response)
            
            # Navigate to Meta careers page
            logger.info(f"Navigating to: {search_url}")
            await self.page.goto(search_url, wait_until='networkidle', timeout=60000)
            
            # Wait for GraphQL to load
            wait_time = self.scraping_config.get("wait_time", 5)
            await self.page.wait_for_timeout(wait_time * 1000)
            
            # Check if we captured the GraphQL response
            if graphql_jobs_data and 'all_jobs' in graphql_jobs_data:
                all_jobs = graphql_jobs_data['all_jobs']
                
                if isinstance(all_jobs, list):
                    logger.info(f"Found {len(all_jobs)} jobs in GraphQL response")
                    
                    for job_data in all_jobs:
                        job = self.meta_parser.parse(job_data)
                        if job:
                            logger.info(f"Parsed job: {job.get('title')} at {job.get('location')}")
                            if self.validate_job_data(job):
                                # Apply location filter
                                if self.matches_location_filter(job):
                                    jobs.append(self.normalize_job_data(job))
                                else:
                                    self.stats["jobs_filtered"] += 1
                                    logger.debug(f'Filtered out job: {job.get("title")} at {job.get("location")}')
                            else:
                                logger.warning(f"Job failed validation: {job}")
                else:
                    logger.warning(f"Unexpected all_jobs format: {type(all_jobs)}")
            else:
                logger.warning("No GraphQL jobs data captured")
            
            self.stats["requests_made"] += 1
            return jobs
            
        except Exception as e:
            logger.error(f"Error scraping Meta GraphQL: {e}")
            self.stats["errors"] += 1
            return []
    
