# Database Integration - Planning Summary

## ğŸ“‹ Overview

This document summarizes the planning for integrating PostgreSQL database into the job scraping platform. The system will track job positions, manage user alerts, and automatically notify users when relevant positions become available.

---

## ğŸ“š Documentation Created

### 1. **PRD_Job_Scraping_Platform.md**
   - Product vision and goals
   - Core features and user stories
   - Success metrics
   - Timeline and phases
   - Risk assessment

### 2. **TECHNICAL_DESIGN.md**
   - System architecture
   - Complete database schema (6 tables)
   - SQLAlchemy model structure
   - Business logic design
   - Daily scraping workflow
   - Migration strategy

### 3. **IMPLEMENTATION_PLAN.md**
   - Step-by-step implementation guide
   - Code examples for all models
   - Alembic migration setup
   - Repository pattern implementation
   - Checklist for each phase

### 4. **Visual Diagrams**
   - System Architecture Diagram
   - Database ERD (Entity Relationship Diagram)
   - Daily Scraping Workflow Sequence Diagram

---

## ğŸ—„ï¸ Database Schema

### Tables Overview

| Table | Purpose | Key Fields |
|-------|---------|------------|
| **users** | User accounts | email, preferences |
| **companies** | Company information | name, careers_url, scraping_config |
| **job_positions** | Job listings | title, location, status, job_url |
| **alerts** | User alert rules | keywords, company_ids, locations |
| **user_job_applications** | Application tracking | user_id, job_position_id, status |
| **alert_notifications** | Notification log | alert_id, job_position_id, delivery_status |

### Key Relationships

```
users (1) ----< (many) alerts
users (1) ----< (many) user_job_applications
users (1) ----< (many) alert_notifications

companies (1) ----< (many) job_positions

job_positions (1) ----< (many) user_job_applications
job_positions (1) ----< (many) alert_notifications

alerts (1) ----< (many) alert_notifications
```

---

## ğŸ”„ Core Workflows

### 1. Daily Scraping Workflow

```
1. Celery scheduler triggers daily task
2. For each active company:
   a. Run scraper â†’ get job listings
   b. Compare with database:
      - New jobs â†’ INSERT with status='active'
      - Existing jobs â†’ UPDATE last_seen_at
      - Missing jobs â†’ UPDATE status='expired'
   c. For new jobs:
      - Find matching user alerts
      - Send notifications
      - Log notifications
   d. Update company.last_scraped_at
3. Generate daily summary report
```

### 2. Alert Matching Logic

```python
# A position matches an alert if ALL of these are true:
- If alert.company_ids specified â†’ position.company_id IN alert.company_ids
- If alert.keywords specified â†’ ANY keyword in position.title
- If alert.excluded_keywords specified â†’ NO keyword in position.title
- If alert.locations specified â†’ position.location matches ANY location
- If alert.departments specified â†’ position.department matches ANY department
- If alert.is_remote specified â†’ position.is_remote == alert.is_remote
```

### 3. Position Lifecycle

```
NEW â†’ ACTIVE â†’ EXPIRED
  â†“      â†“         â†“
  |      |         â””â”€> No longer on career page
  |      â””â”€> Still on career page (update last_seen_at)
  â””â”€> First time seen (create record)
```

---

## ğŸ—ï¸ System Architecture

### Layers

1. **External Sources Layer**
   - Company career pages
   - ATS APIs (Greenhouse, Comeet, Workday)

2. **Scraping Layer**
   - Playwright scraper
   - API scrapers (Greenhouse, Comeet, etc.)

3. **Task Queue Layer**
   - Celery workers
   - Redis queue
   - Scheduled tasks

4. **Business Logic Layer**
   - Position Lifecycle Manager
   - Alert Matcher
   - Notification Service

5. **Data Access Layer**
   - SQLAlchemy ORM
   - Repository pattern

6. **Database Layer**
   - PostgreSQL (local + remote)

---

## ğŸ“¦ Technology Stack

| Component | Technology | Version |
|-----------|-----------|---------|
| Database | PostgreSQL | 14+ |
| ORM | SQLAlchemy | 2.0.23 |
| Migrations | Alembic | 1.12.1 |
| DB Driver | psycopg2-binary | 2.9.9 |
| Async DB | asyncpg | 0.29.0 |
| Task Queue | Celery + Redis | (existing) |
| Web Scraping | Playwright | (existing) |

---

## ğŸ“… Implementation Timeline

### Phase 1: Database Setup (Week 1)
- [ ] Install PostgreSQL locally
- [ ] Create database schema
- [ ] Set up Alembic migrations
- [ ] Create SQLAlchemy models
- [ ] Test CRUD operations

### Phase 2: Data Migration (Week 1-2)
- [ ] Migrate companies from YAML to DB
- [ ] Create repositories
- [ ] Update scrapers to use DB config
- [ ] Test scraping with DB

### Phase 3: Position Lifecycle (Week 2)
- [ ] Implement PositionLifecycleManager
- [ ] Integrate with scraping tasks
- [ ] Test new/expired detection
- [ ] Add monitoring

### Phase 4: User & Alert System (Week 3)
- [ ] Implement User/Alert models
- [ ] Create AlertMatcher engine
- [ ] Implement email notifications
- [ ] Test end-to-end flow

### Phase 5: Production Deployment (Week 4)
- [ ] Set up remote PostgreSQL
- [ ] Run production migrations
- [ ] Deploy workers
- [ ] Set up monitoring
- [ ] Create backup strategy

---

## ğŸ¯ Key Features

### For Users
âœ… **Personalized Alerts**: Set up alerts for specific companies, roles, and locations  
âœ… **New Position Notifications**: Get notified immediately when matching jobs are posted  
âœ… **Expiration Tracking**: Know when positions are removed (likely filled)  
âœ… **Application Tracking**: Track which positions you've applied to  
âœ… **Multi-company Monitoring**: Monitor 35+ tech companies simultaneously  

### For System
âœ… **Automated Scraping**: Daily scraping of all active companies  
âœ… **Smart Detection**: Automatically detect new and expired positions  
âœ… **Reliable Notifications**: Email delivery with retry logic  
âœ… **Audit Trail**: Complete history of all notifications sent  
âœ… **Scalable Architecture**: Support for 1000+ users and 3000+ positions  

---

## ğŸ” Security & Best Practices

### Database Security
- SSL/TLS connections
- Credential rotation
- Connection pooling
- Query optimization with indexes

### Data Privacy
- Minimal user data collection
- GDPR compliance ready
- Secure password hashing (if auth added)
- API rate limiting

### Scraping Ethics
- Respect robots.txt
- Rate limiting per company
- Appropriate user agents
- No server overload

---

## ğŸ“Š Success Metrics

### Technical Metrics
- **Scraping Success Rate**: >95%
- **Data Freshness**: <24 hours
- **System Uptime**: >99.5%
- **Alert Delivery Time**: <5 minutes

### User Metrics
- **Alert Accuracy**: >98% relevant
- **Position Detection**: >99% within 24h
- **Notification Delivery**: >99% success rate

---

## ğŸš€ Next Steps

### Immediate Actions
1. **Review** all planning documents
2. **Answer** open questions (database hosting, email service, etc.)
3. **Set up** local PostgreSQL
4. **Create** initial database schema
5. **Implement** SQLAlchemy models

### Questions to Answer
1. Where to host production database? (AWS RDS / Cloud SQL / DigitalOcean)
2. Which email service for notifications? (SendGrid / AWS SES / Mailgun)
3. Do we need user authentication in v1? (Yes/No)
4. Data retention policy for expired positions? (30/90 days / forever)
5. Alert frequency preference? (Immediate / Daily digest / Configurable)

---

## ğŸ“ File Structure (After Implementation)

```
scrapper/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/                    # SQLAlchemy models
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”œâ”€â”€ user.py
â”‚   â”‚   â”œâ”€â”€ company.py
â”‚   â”‚   â”œâ”€â”€ job_position.py
â”‚   â”‚   â”œâ”€â”€ alert.py
â”‚   â”‚   â”œâ”€â”€ user_job_application.py
â”‚   â”‚   â””â”€â”€ alert_notification.py
â”‚   â”œâ”€â”€ repositories/              # Data access layer
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_repository.py
â”‚   â”‚   â”œâ”€â”€ company_repository.py
â”‚   â”‚   â”œâ”€â”€ job_position_repository.py
â”‚   â”‚   â”œâ”€â”€ user_repository.py
â”‚   â”‚   â””â”€â”€ alert_repository.py
â”‚   â”œâ”€â”€ services/                  # Business logic
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ position_lifecycle_manager.py
â”‚   â”‚   â”œâ”€â”€ alert_matcher.py
â”‚   â”‚   â””â”€â”€ notification_service.py
â”‚   â””â”€â”€ database/                  # Database utilities
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ connection.py
â”‚       â””â”€â”€ session.py
â”œâ”€â”€ migrations/                    # Alembic migrations
â”‚   â””â”€â”€ versions/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ database.py               # Database configuration
â”‚   â””â”€â”€ companies.yaml            # (will migrate to DB)
â”œâ”€â”€ docs/                         # Documentation
â”‚   â”œâ”€â”€ PRD_Job_Scraping_Platform.md
â”‚   â”œâ”€â”€ TECHNICAL_DESIGN.md
â”‚   â”œâ”€â”€ IMPLEMENTATION_PLAN.md
â”‚   â””â”€â”€ DATABASE_INTEGRATION_SUMMARY.md
â””â”€â”€ alembic.ini                   # Alembic config
```

---

## ğŸ’¡ Design Decisions

### Why PostgreSQL?
- âœ… Robust ACID compliance
- âœ… Excellent JSON support (JSONB)
- âœ… Array data types for alert filters
- âœ… Full-text search capabilities
- âœ… Mature ecosystem and tooling

### Why SQLAlchemy?
- âœ… Industry-standard Python ORM
- âœ… Excellent migration support (Alembic)
- âœ… Type safety and IDE support
- âœ… Async support for scalability
- âœ… Repository pattern compatibility

### Why Repository Pattern?
- âœ… Separation of concerns
- âœ… Easier testing (mock repositories)
- âœ… Consistent data access interface
- âœ… Business logic isolation
- âœ… Future-proof for API layer

---

## ğŸ“ Learning Resources

### PostgreSQL
- [PostgreSQL Official Docs](https://www.postgresql.org/docs/)
- [PostgreSQL Tutorial](https://www.postgresqltutorial.com/)

### SQLAlchemy
- [SQLAlchemy 2.0 Documentation](https://docs.sqlalchemy.org/en/20/)
- [SQLAlchemy ORM Tutorial](https://docs.sqlalchemy.org/en/20/orm/tutorial.html)

### Alembic
- [Alembic Documentation](https://alembic.sqlalchemy.org/)
- [Alembic Tutorial](https://alembic.sqlalchemy.org/en/latest/tutorial.html)

---

## âœ… Planning Complete!

All planning documents have been created and are ready for review. The next step is to begin implementation starting with Phase 1: Database Setup.

**Ready to proceed?** Start with:
```bash
# Install PostgreSQL
brew install postgresql@14

# Create database
createdb job_scraper_dev

# Install Python dependencies
pip install sqlalchemy psycopg2-binary alembic asyncpg
```

